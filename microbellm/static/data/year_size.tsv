Model	Organization	Publication date	Reference	Parameters	Training compute (FLOP)	Training compute notes	Training dataset	Approach	Confidence	Abstract	Epochs	Benchmark data	Model accessibility	Country (of organization)	Last modified	Training cloud compute vendor	Training data center	Archived links	Batch size	Batch size notes	Organization categorization
GLM-4.5	Zhipu AI,Tsinghua University	2025-08-05	GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models	355000000000	4.42E+24	(6 FLOP/token/parameter) * (23T tokens) * (32B active parameters) = 4.42e24 FLOP			Confident	We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.			Open weights (unrestricted)	China,China	2025-09-12 16:12:01+00:00						Industry,Academia
GLM-4V-9B	Zhipu AI,Tsinghua University	2024-06-18	ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools	9000000000		6ND = 6 FLOP / token / parameter * 9000000000 parameters * 10000000000000 tokens = 54*10^(9+13)=5.4*10^23 FLOP  "GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)". 			Likely	We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.			Open weights (non-commercial)	China,China	2025-09-12 16:18:30+00:00						Industry,Academia
GLM-4	Zhipu AI	2024-01-17	ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools						Confident	We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.			Hosted access (no API)	China	2025-09-12 16:18:30+00:00						Industry
grok-4	xAI	2025-07-09	Grok 4		5.00E+26	We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:  2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4) 1.1 * (grok3/1.01) in the low case The geometric mean is (rounded to one sig fig): 5e26 	Unspecified unreleased		Speculative	Grok 4 is the most intelligent model in the world. It includes native tool use and real-time search integration, and is available now to SuperGrok and Premium+ subscribers, as well as through the xAI API. We are also introducing a new SuperGrok Heavy tier with access to Grok 4 Heavy - the most powerful version of Grok 4.			API access	United States of America	2025-09-12 17:32:20+00:00		"we utilized Colossus, our 200,000 GPU cluster, to run reinforcement learning training that refines Grok's reasoning abilities at pretraining scale"				Industry
Grok-3-mini	xAI	2025-02-19					Unspecified unreleased		Unknown				Hosted access (no API)	United States of America	2025-08-22 13:47:44+00:00						Industry
Grok-2	xAI	2024-08-13	Grok-2 Beta Release		2.96E+25	Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing	Unspecified unreleased		Confident	Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the ùïè platform.			Hosted access (no API)	United States of America	2025-08-11 14:22:24+00:00						Industry
Grok-2-mini	xAI	2024-08-13	Grok-2 Beta Release				Unspecified unreleased		Unknown	Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the ùïè platform.			Hosted access (no API)	United States of America	2025-05-13 05:38:48+00:00						Industry
OpenChat-13b	Tsinghua University,Shanghai AI Lab,01.AI	2023-09-20	OpenChat: Advancing Open-source Language Models with Mixed-Quality Data	13000000000	7.81E+22	The developers only fine-tuned Llama-2-13b, according to arxiv.org/pdf/2309.11235.  Finetune FLOP: 5.23e19 Base model FLOP: 7.8e+22 Total: 78052170300000010000000			Confident	Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.	5		Open weights (unrestricted)	China,China,China	2025-08-01 16:23:10+00:00						Academia,Academia,Industry
GLM-4-9B-0414	Tsinghua University	2025-04-14	GLM-4-9B-0414	9000000000	8.10E+23	Assuming it was trained on the same 15T dataset as 32B model:  6 FLOP / parameter / token * 9 * 10^9 parameters * 15 * 10^12 tokens = 8.1e+23 FLOP   "Likely" confidence due to the uncertain dataset size	Unspecified unreleased		Likely	Finally, GLM-Z1-9B-0414 is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.			Open weights (unrestricted)	China	2025-08-11 14:56:58+00:00						Academia
Sonar	Perplexity	2025-02-11	Meet new Sonar: A Blazing Fast Model Optimized for Perplexity Search	70000000000			Unspecified unreleased		Confident	Starting today, all Perplexity Pro users will be able to try out the latest version of Sonar, Perplexity's in-house model that is optimized for answer quality and user experience. Built on top of Llama 3.3 70B, Sonar has been further trained to enhance answer factuality and readability for Perplexity‚Äôs default search mode.  Through comprehensive online A/B testing, we have found that Sonar significantly outperforms models in its class, like GPT-4o mini and Claude 3.5 Haiku, while closely matching or exceeding the performance of frontier models like GPT-4o and Claude 3.5 Sonnet for user satisfaction. Powered by Cerebras inference infrastructure, Sonar runs at a blazing fast speed of 1200 tokens per second ‚Äî enabling nearly instant answer generation.			API access	United States of America	2025-06-11 14:38:36+00:00						Industry
GPT-4b-micro	OpenAI,Retro Biosciences	2025-01-17	OpenAI‚Äôs new model, called GPT-4b micro, was trained to suggest ways to re-engineer the protein factors to increase their function.			GPT-4b is a micro model compared to the other ChatGPT options as it was only trained on protein data.	Unspecified unreleased		Unknown	OpenAI‚Äôs new model, called GPT-4b micro, was trained to suggest ways to re-engineer the protein factors to increase their function. According to OpenAI, researchers used the model‚Äôs suggestions to change two of the Yamanaka factors to be more than 50 times as effective‚Äîat least according to some preliminary measures.   ‚ÄúJust across the board, the proteins seem better than what the scientists were able to produce by themselves,‚Äù says John Hallman, an OpenAI researcher.			Unreleased	United States of America,United States of America	2025-05-01 14:42:42+00:00						Industry,Industry
GPT-5	OpenAI	2025-08-07	GPT-5 System Card						Unknown					United States of America	2025-08-22 14:16:42+00:00						Industry
gpt-5-mini	OpenAI	2025-08-07	GPT-5 System Card						Unknown					United States of America	2025-08-22 14:16:58+00:00						Industry
gpt-5-nano	OpenAI	2025-08-07	GPT-5 System Card						Unknown					United States of America	2025-08-22 14:17:04+00:00						Industry
gpt-oss-120b	OpenAI	2025-08-05	gpt-oss-120b & gpt-oss-20b Model Card	116830000000	4.94E+24	"The training run for gpt-oss-120b required 2.1 million H100-hours to complete" (2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24 They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24  			Confident				Open weights (unrestricted)	United States of America	2025-08-11 14:56:58+00:00						Industry
gpt-oss-20b	OpenAI	2025-08-05	gpt-oss-120b & gpt-oss-20b Model Card	20910000000	5.49E+23	"The training run for gpt-oss-120b required 2.1 million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer" assuming "almost 10x fewer" means ~9x fewer: 4.94e24/9 = 5.49e23			Confident				Open weights (unrestricted)	United States of America	2025-08-11 14:56:58+00:00						Industry
o3-pro	OpenAI	2025-06-10							Unknown					United States of America	2025-08-11 14:56:58+00:00						Industry
o4-mini	OpenAI	2025-04-16	Introducing OpenAI o3 and o4-mini: Our smartest and most capable models to date with full tool access			We can‚Äôt make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.	Unspecified unreleased		Unknown	Today, we‚Äôre releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models we‚Äôve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT‚Äîthis includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness. <..> OpenAI o4-mini is a smaller model optimized for fast, cost-efficient reasoning‚Äîit achieves remarkable performance for its size and cost, particularly in math, coding, and visual tasks. It is the best-performing benchmarked model on AIME 2024 and 2025. Although access to a computer meaningfully reduces the difficulty of the AIME exam, we also found it notable that o4-mini achieves 99.5% pass@1 (100% consensus@8) on AIME 2025 when given access to a Python interpreter. While these results should not be compared to the performance of models without tool access, they are one example of how effectively o4-mini leverages available tools; o3 shows similar improvements on AIME 2025 from tool use (98.4% pass@1, 100% consensus@8).			API access	United States of America	2025-07-01 12:36:40+00:00						Industry
GPT-4.1	OpenAI	2025-04-14	Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long context‚Äîplus our first-ever nano model.			Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.	Unspecified unreleased		Unknown	Today, we‚Äôre launching three new models in the API: GPT‚Äë4.1, GPT‚Äë4.1 mini, and GPT‚Äë4.1 nano. These models outperform GPT‚Äë4o and GPT‚Äë4o mini across the board, with major gains in coding and instruction following. They also have larger context windows‚Äîsupporting up to 1 million tokens of context‚Äîand are able to better use that context with improved long-context comprehension. They feature a refreshed knowledge cutoff of June 2024.  GPT‚Äë4.1 excels at the following industry standard measures:   Coding: GPT‚Äë4.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPT‚Äë4o and 26.6%abs over GPT‚Äë4.5‚Äîmaking it a leading model for coding. Instruction following: On Scale‚Äôs MultiChallenge‚Å†(opens in a new window) benchmark, a measure of instruction following ability, GPT‚Äë4.1 scores 38.3%, a 10.5%abs increase over GPT‚Äë4o. Long context: On Video-MME‚Å†(opens in a new window), a benchmark for multimodal long context understanding, GPT‚Äë4.1 sets a new state-of-the-art result‚Äîscoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT‚Äë4o.			API access	United States of America	2025-06-30 17:51:55+00:00						Industry
GPT-4.1-mini	OpenAI	2025-04-14	Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long context‚Äîplus our first-ever nano model.				Unspecified unreleased		Unknown	GPT‚Äë4.1 mini is a significant leap in small model performance, even beating GPT‚Äë4o in many benchmarks. It matches or exceeds GPT‚Äë4o in intelligence evals while reducing latency by nearly half and reducing cost by 83%. 			API access	United States of America	2025-05-30 18:50:46+00:00						Industry
GPT-4.1-nano	OpenAI	2025-04-14	Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long context‚Äîplus our first-ever nano model.				Unspecified unreleased		Unknown	For tasks that demand low latency, GPT‚Äë4.1 nano is our fastest and cheapest model available. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding‚Äîeven higher than GPT‚Äë4o mini. It‚Äôs ideal for tasks like classification or autocompletion.			API access	United States of America	2025-05-30 18:50:47+00:00						Industry
GPT-4.5	OpenAI	2025-02-27	Introducing GPT-4.5		2.10E+26	Speculative estimate based on GPT-N typically incrementing  by about 100x each version number (https://www.youtube.com/watch?v=6nJZopACRuQ) and 10x each 0.5 version number (https://x.com/karpathy/status/1895213020982472863).  OpenAI said GPT-4.5 was a ‚Äúnew order of magnitude in compute‚Äù, which they could have meant somewhat loosely. But seems quite likely >1e26 based on that statement, plus 4.5's high inference costs, and <5e26 because OpenAI probably didn‚Äôt have a substantially >100k H100 cluster in mid-2024.  In the "Pretraining GPT-4.5" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 	Unspecified unreleased		Speculative	We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.  Unsupervised learning increases world model accuracy and intuition. Models like GPT‚Äë3.5, GPT‚Äë4, and GPT‚Äë4.5 advance this paradigm. Scaling reasoning‚Å†, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‚Äëmini advance this paradigm. GPT‚Äë4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‚Äë4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.			API access	United States of America	2025-08-01 16:23:42+00:00	Azure AI					Industry
o3-mini	OpenAI	2025-01-31	Pushing the frontier of cost-effective reasoning.			We can‚Äôt make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.	Unspecified unreleased		Unknown	We‚Äôre releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024‚Å†, this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilities‚Äîwith particular strength in science, math, and coding‚Äîall while maintaining the low cost and reduced latency of OpenAI o1-mini.  OpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function calling‚Å†(opens in a new window), Structured Outputs‚Å†(opens in a new window), and developer messages‚Å†(opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streaming‚Å†(opens in a new window). Also, developers can choose between three reasoning effort‚Å†(opens in a new window) options‚Äîlow, medium, and high‚Äîto optimize for their specific use cases. This flexibility allows o3-mini to ‚Äúthink harder‚Äù when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5‚Å†			API access	United States of America	2025-07-01 12:36:41+00:00						Industry
o3	OpenAI	2024-12-20	Our most powerful reasoning model with leading performance on coding, math, science, and vision				Unspecified unreleased		Unknown	Today, we‚Äôre releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models we‚Äôve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT‚Äîthis includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness. <..> OpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. It‚Äôs ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasks‚Äîespecially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypotheses‚Äîparticularly within biology, math, and engineering contexts.  ________ model was announced 2024/12/20  from ARS technica: "On Friday, during Day 12 of its "12 days of OpenAI," OpenAI CEO Sam Altman announced its latest AI "reasoning" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today."  https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/  model was released 2025/04/16			API access	United States of America	2025-06-25 18:41:17+00:00						Industry
o1	OpenAI	2024-12-05	Introducing ChatGPT Pro: Broadening usage of frontier AI.				Unspecified unreleased		Unknown	We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.  Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we‚Äôre also including evaluations for the next update, currently in development.			API access	United States of America	2025-08-19 20:56:36+00:00						Industry
o1-mini	OpenAI	2024-09-12	Learning to reason with LLMs			We can‚Äôt make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.	Unspecified unreleased		Unknown	We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.  ...  We‚Äôre also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.			API access	United States of America	2025-08-11 14:22:24+00:00						Industry
gpt-4o-mini	OpenAI	2024-07-18	GPT-4o mini: advancing cost-efficient intelligence		7.36E+24	Training compute estimated from benchmark scores.  90% CI [3.23e+24, 2.05e+25]	Unspecified unreleased		Speculative	OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.			API access	United States of America	2025-08-11 14:22:24+00:00						Industry
Hermes 3 405B	Nous Research	2024-08-14	Hermes 3 Technical Report	405000000000			Hermes 3		Confident	Instruct (or ‚Äúchat‚Äù) tuned models have become the primary way in which most people interact with large language models. As opposed to ‚Äúbase‚Äù or ‚Äúfoundation‚Äù models, instruct-tuned models are optimized to respond to imperative statements. We present Hermes 3, a neutrally-aligned generalist instruct and tool use model with strong reasoning and creative abilities. Its largest version, Hermes 3 405B, achieves state of the art performance among open weight models on several public benchmarks. The weights for all models are available at https://huggingface.co/NousResearch.	4		Open weights (restricted use)	United States of America	2025-06-24 13:12:29+00:00						Industry
Hermes 3 70B	Nous Research	2024-08-14	Hermes 3 Technical Report	70000000000			Hermes 3		Confident	Instruct (or ‚Äúchat‚Äù) tuned models have become the primary way in which most people interact with large language models. As opposed to ‚Äúbase‚Äù or ‚Äúfoundation‚Äù models, instruct-tuned models are optimized to respond to imperative statements. We present Hermes 3, a neutrally-aligned generalist instruct and tool use model with strong reasoning and creative abilities. Its largest version, Hermes 3 405B, achieves state of the art performance among open weight models on several public benchmarks. The weights for all models are available at https://huggingface.co/NousResearch.	3		Open weights (restricted use)	United States of America	2025-06-24 13:12:29+00:00						Industry
Hermes 3 8B	Nous Research	2024-08-14	Hermes 3 Technical Report	8000000000			Hermes 3		Confident	Instruct (or ‚Äúchat‚Äù) tuned models have become the primary way in which most people interact with large language models. As opposed to ‚Äúbase‚Äù or ‚Äúfoundation‚Äù models, instruct-tuned models are optimized to respond to imperative statements. We present Hermes 3, a neutrally-aligned generalist instruct and tool use model with strong reasoning and creative abilities. Its largest version, Hermes 3 405B, achieves state of the art performance among open weight models on several public benchmarks. The weights for all models are available at https://huggingface.co/NousResearch.	4		Open weights (restricted use)	United States of America	2025-06-24 13:12:29+00:00						Industry
Kimi-K2	Moonshot	2025-07-11	Kimi K2: Open Agentic Intelligence	1000000000000	2.98E+24	6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP	Unspecified unreleased		Confident	Kimi K2 is our latest Mixture-of-Experts model with 32 billion activated parameters and 1 trillion total parameters. It achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. But it goes further ‚Äî meticulously optimized for agentic tasks, Kimi K2 does not just answer; it acts. And now, it is within your reach. Today, we are open-sourcing: Kimi-K2-Base: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions. Kimi-K2-Instruct: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking. With Kimi K2, advanced agentic intelligence is more open and accessible than ever. We can't wait to see what you build.			Open weights (restricted use)	China	2025-08-11 17:19:30+00:00						Industry
Kimi-1.6	Moonshot	2025-02-27					Unspecified unreleased		Unknown				Unreleased	China	2025-07-14 07:40:55+00:00						Industry
Mistral-Small-3.1	Mistral AI	2025-03-17	Mistral Small 3.1: the best model in its weight class.	24000000000		At least 1.152e+24 FLOP (base model Mistral Small 3 training compute)	Unspecified unreleased		Confident	Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second. Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.  Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential.   Low-latency function calling: Capable of rapid function execution within automated or agentic workflows  Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.  Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.  Mistral Small 3.1 can be used across various enterprise and consumer applications that require multimodal understanding, such as document verification, diagnostics, on-device image processing, visual inspection for quality checks, object detection in security systems, image-based customer support, and general purpose assistance.			Open weights (unrestricted)	France	2025-07-02 12:16:12+00:00						Industry
Mistral-Small-3	Mistral AI	2025-01-30	Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.	24000000000	1.15E+24	6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP	Unspecified unreleased		Confident	Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.  Mistral Small 3 is a pre-trained and instructed model catered to the ‚Äò80%‚Äô of generative AI tasks‚Äîthose that require robust language and instruction following performance, with very low latency.  We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.  We‚Äôre releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.			Open weights (unrestricted)	France	2025-08-01 16:23:40+00:00						Industry
Ministral-3B	Mistral AI	2024-10-16	Un Ministral, des Ministraux Introducing the world‚Äôs best edge models.	3000000000			Unspecified unreleased		Confident	On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.  These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.  Ministral 3B is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have real-time requirements and high-volume.			API access	France	2025-08-11 14:22:24+00:00						Industry
Ministral-8B	Mistral AI	2024-10-16	Un Ministral, des Ministraux Introducing the world‚Äôs best edge models.	8000000000			Unspecified unreleased		Confident	On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.  These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.			Open weights (non-commercial)	France	2025-08-11 14:22:24+00:00						Industry
mistral-nemo	Mistral AI	2024-07-18	Mistral NeMo	12000000000				Self-supervised learning	Confident	Mistral NeMo: our new best small model. A state-of-the-art 12B model with 128k context length, built in collaboration with NVIDIA, and released under the Apache 2.0 license.			Open weights (unrestricted)	France	2025-08-11 14:22:24+00:00						Industry
Mixtral-8x22B	Mistral AI	2024-04-17	Mixtral 8x22B	141000000000	2.34E+24	Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with "speculative" confidence:  6 FLOP / token / parameter * 39 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 2.34e+24 FLOP	Unspecified unreleased	Self-supervised learning	Speculative	Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.  Mixtral 8x22B comes with the following strengths:  - It is fluent in English, French, Italian, German, and Spanish - It has strong mathematics and coding capabilities - It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale - Its 64K tokens context window allows precise information recall from large documents 			Open weights (unrestricted)	France	2025-08-11 14:22:24+00:00						Industry
Mistral-7B	Mistral AI	2023-10-10	Mistral 7B	7000000000			Unspecified unreleased		Confident	We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.			Open weights (unrestricted)	France	2025-08-11 14:22:24+00:00						Industry
Phi-4	Microsoft Research	2024-12-12	Phi-4 Technical Report	14000000000	9.32E+23	6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP  989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP  geometric mean sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23	Unspecified unreleased		Confident	We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.			Open weights (unrestricted)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-01 16:23:39+00:00						Industry
Phi-4-Reasoning	Microsoft	2025-04-30	Phi-4-reasoning Technical Report	14000000000	9.34E+23	9.3202015e+23 FLOP [base model compute] + 1.6606191e+21 FLOP [finetune compute] = 9.3368077e+23 FLOP	Unspecified unreleased		Confident	We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.	1.93		Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-11 14:56:58+00:00						Industry
Phi-4-Reasoning-plus	Microsoft	2025-04-30	Phi-4-reasoning Technical Report	14000000000			Unspecified unreleased		Confident	We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-11 14:56:58+00:00						Industry
Phi-4-Mini	Microsoft	2025-03-03	Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs	3800000000	9.96E+22	6ND = 6 FLOP / token / parameter * 3800000000 parameters * 5000000000000 tokens = 1.14e+23 FLOP  512 GPUs * 312000000000000 FLOP / sec * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 8.6951854e+22 FLOP  geometric mean:  sqrt(1.14e+23*8.6951854e+22) = 9.9561596e+22	Unspecified unreleased		Confident	We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-05 19:36:29+00:00						Industry
Phi-3.5-MoE	Microsoft	2024-04-23	Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone	60800000000	3.02E+23	512 GPUs * 989500000000000 FLOP / sec * 552 hours * 3600 sec / hour * 0.3 [assumed utilization] = 3.0202896e+23 FLOP  6 FLOP / token / parameter * 4900000000000 tokens * 6.6*10^9 active parameters = 1.9404e+23 FLOP (slightly less confidence than hardware estimation since the 6ND formula is less accurate for MoE)	Unspecified unreleased		Confident	We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-05 19:36:15+00:00						Industry
WizardLM-2-8x22B	Microsoft	2024-04-15		141000000000					Confident	We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.  WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.  WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models. 			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-11 14:22:24+00:00						Industry
WizardLM-2-70B	Microsoft	2024-04-15		70000000000					Confident	We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.  WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.  WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models. 			Unreleased	United States of America,Multinational,India,Belgium	2025-05-01 14:42:42+00:00						Industry
WizardLM-2-7B	Microsoft	2024-04-15		7000000000					Confident	We introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, which have improved performance on complex chat, multilingual, reasoning and agent. New family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.  WizardLM-2 is the latest milestone in our effort in scaling up LLM post-training. One year ago, we have been iterating on training of Wizard series since our first work on Empowering Large Language Models to Follow Complex Instructions, then we accelerate the evolution to code and math reasoning scenarios. Since then, Evol-Instruct and Instruction&Process Supervised Reinforcement Learning (RLEIF) have become fundamental technologies for GenAI community. Recently, we have further optimized our methods and data quality, resulting in significant performance improvements, the outcome is WizardLM-2.  WizardLM-2 8x22B is our most advanced model, and the best opensource LLM in our internal evaluation on highly complex tasks. WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models. 			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-05-12 19:41:07+00:00						Industry
Llama-4-Scout	Meta AI	2025-04-05	The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation	109000000000	4.08E+24	40T training tokens per model card:  https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md    Estimating training compute from parameters and tokens: 6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP (Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)   The model card also states that Llama 4 Scout used 5.0M H100-hours. The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have: Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP (This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)			Likely	We‚Äôre sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences. Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks. Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding‚Äîat less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena. These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world‚Äôs smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we‚Äôre excited to share more details about it even while it‚Äôs still in flight. Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.			Open weights (restricted use)	United States of America	2025-08-04 17:00:48+00:00						Industry
Llama-4-Maverick	Meta AI	2025-04-05	The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation	400000000000	2.24E+24	22T training tokens per model card:  https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md     Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.  Estimating training compute from parameters and tokens: Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP (Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)   The model card also states that Llama 4 Maverick used 2.38M H100-hours. The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have: Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP (This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)			Likely	We‚Äôre sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences. Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks. Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding‚Äîat less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena. These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world‚Äôs smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we‚Äôre excited to share more details about it even while it‚Äôs still in flight. Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.			Open weights (restricted use)	United States of America	2025-08-04 17:00:51+00:00						Industry
Llama-3.3-70B	Meta AI	2024-12-06	Meta Llama 3.3 multilingual large language model (LLM)	70000000000	6.86E+24	6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP  7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP  sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24	Unspecified unreleased		Confident	The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.  Model developer: Meta  Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.			Open weights (restricted use)	United States of America	2025-08-01 16:23:38+00:00						Industry
Llama-3.2-11B	Meta AI	2024-09-24	Llama 3.2: Revolutionizing edge AI and vision with open, customizable models	10600000000	5.79E+23	Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).  ‚ÄúTraining utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency‚Ä¶ Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.‚Äù (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).  The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).  Assuming 33% utilization rate, Training compute ~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU ~= 5.79e23 FLOPS	Unspecified unreleased		Confident	Today, we‚Äôre releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions. Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They‚Äôre also available to try using our smart assistant, Meta AI. We‚Äôre sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. We‚Äôve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama. We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency‚Äîenabling more people to have creative, useful, and life-changing breakthroughs using generative AI. We‚Äôre making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.			Open weights (restricted use)	United States of America	2025-08-05 19:36:24+00:00						Industry
Llama-3.2-90B	Meta AI	2024-09-24	Llama 3.2: Revolutionizing edge AI and vision with open, customizable models	88600000000			Unspecified unreleased		Confident	Today, we‚Äôre releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions. Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They‚Äôre also available to try using our smart assistant, Meta AI. We‚Äôre sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. We‚Äôve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama. We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency‚Äîenabling more people to have creative, useful, and life-changing breakthroughs using generative AI. We‚Äôre making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.			Open weights (restricted use)	United States of America	2025-05-30 18:36:29+00:00						Industry
Llama-3.2-1B	Meta AI	2024-09-24	Llama 3.2: Revolutionizing edge AI and vision with open, customizable models	1230000000	6.64E+22	6ND = 6*1230000000.00*9000000000000 = 6.642e+22  370000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 5.346648e+22	Unspecified unreleased		Confident	Today, we‚Äôre releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions. The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors. We‚Äôre sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. We‚Äôve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama. We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency‚Äîenabling more people to have creative, useful, and life-changing breakthroughs using generative AI. We‚Äôre making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.			Open weights (restricted use)	United States of America	2025-08-05 19:36:24+00:00						Industry
Llama-3.2-3B	Meta AI	2024-09-24	Llama 3.2: Revolutionizing edge AI and vision with open, customizable models	3210000000	1.73E+23	6ND = 6*3210000000.00*9000000000000 = 1.7334e+23  460000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 6.647184e+22	Unspecified unreleased		Confident	Today, we‚Äôre releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions. The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors. We‚Äôre sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. We‚Äôve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama. We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency‚Äîenabling more people to have creative, useful, and life-changing breakthroughs using generative AI. We‚Äôre making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.			Open weights (restricted use)	United States of America	2025-08-01 16:23:35+00:00						Industry
Llama 3.1-70B	Meta AI	2024-07-23	The Llama 3 Herd of Models	70000000000	7.93E+24	Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%  6ND: 6 * 15T * 70B = 6.3e24 FLOPs  Hardware: 7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs  Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24  Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.  Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP	Llama 3 dataset		Confident	Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.			Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
Llama-3.1-8B	Meta AI	2024-07-23	The Llama 3 Herd of Models	8000000000	1.22E+24	Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%  6ND: 6 * 15T * 8B = 7.2e23 FLOPs  Hardware: 1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs  Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24  Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.	Llama 3 dataset		Likely	Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.			Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
Llama-3-70B	Meta AI	2024-04-18	Introducing Meta Llama 3: The most capable openly available LLM to date	70000000000	7.86E+24	Arithmetic calculation: 6 * 15T tokens * 70B parameters = 6.3e24  GPU calculation: https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one). 990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24  Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24	Llama 3 dataset		Confident				Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
llama-3-8b-instruct	Meta AI	2024-04-18	Introducing Meta Llama 3: The most capable openly available LLM to date	8000000000	7.20E+23	Counting operations 15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2√ó10^23 FLOP  GPU calculation 400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872√ó10^24  (it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)	Llama 3 dataset		Confident	Today, we‚Äôre introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm. We‚Äôre dedicated to developing Llama 3 in a responsible way, and we‚Äôre offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2. In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we‚Äôll share the Llama 3 research paper. Meta AI, built with Llama 3 technology, is now one of the world‚Äôs leading AI assistants that can boost your intelligence and lighten your load‚Äîhelping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.			Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
Llama-2-70B	Meta AI	2023-07-18	Llama 2: Open Foundation and Fine-Tuned Chat Models	70000000000	8.10E+23	"Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB" of which 1720320 GPU hours were used to train the 70B model.  311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.  Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.	Llama 2 dataset	Supervised	Confident	In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.	1		Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00		Meta‚Äôs Research Super Cluster		4000000		Industry
Llama 2-34B	Meta AI	2023-07-18	Llama 2: Open Foundation and Fine-Tuned Chat Models	34000000000	4.08E+23	All models sizes trained on 2.0T tokens, per table 1 2T * 34b * 6 = 4.08e23  Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.	Llama 2 dataset	Supervised	Confident	In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.	1		Unreleased	United States of America	2025-08-01 16:23:06+00:00		Meta‚Äôs Research Super Cluster		4000000		Industry
Llama 2-7B	Meta AI	2023-07-18	Llama 2: Open Foundation and Fine-Tuned Chat Models	7000000000	8.40E+22	Trained on 2 trillion tokens per Table 1.   C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.  Also, 7B model was trained on 184320 GPU-hours  312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP	Llama 2 dataset	Supervised	Confident	In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.	1		Open weights (restricted use)	United States of America	2025-08-01 16:23:06+00:00		Meta‚Äôs Research Super Cluster		4000000		Industry
Llama 2-13B	Meta AI	2023-07-18	Llama 2: Open Foundation and Fine-Tuned Chat Models	13000000000	1.60E+23	13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP	Llama 2 dataset	Supervised	Confident	In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.	1		Open weights (restricted use)	United States of America	2025-08-07 06:15:59+00:00		Meta‚Äôs Research Super Cluster		4000000		Industry
Gemini-Flash-2.0:Thinking	Google DeepMind,Google	2024-12-19	Our enhanced reasoning model, capable of showing its thoughts to improve performance and explainability				Unspecified unreleased		Unknown	Combining speed and performance, 2.0 Flash Thinking Experimental also excels in science and math, showing its thinking to solve complex problems.  Enhanced performance Improvements on math and science benchmarks.  Long context A one-million token context window enables deeper analysis of long-form text.  Improved thinking More consistency between thoughts and answers.  Tool use Turn on code execution to run and evaluate code.  Best for Complex tasks without the need for low latency			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland,Germany	2025-06-25 18:31:07+00:00						Industry,Industry
Gemini-Flash-2.0 	Google DeepMind,Google	2024-12-11	Introducing Gemini 2.0: our new AI model for the agentic era			"We used Trillium TPUs to train the new Gemini 2.0, Google‚Äôs most capable AI model yet" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga	Unspecified unreleased		Unknown	Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland,Germany	2025-06-10 16:49:19+00:00						Industry,Industry
gemma-3	Google DeepMind	2025-08-14	Introducing Gemma 3 270M: The compact model for hyper-efficient AI	270000000	9.72E+21	6 FLOP / parameter / token * 270 * 10^9 parameters * 6 * 10^12 tokens = 9.72e+21 FLOP	Unspecified unreleased		Confident	Today, we're adding a new, highly specialized tool to the Gemma 3 toolkit: Gemma 3 270M, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in. Compact and capable architecture: Our new model has a total of 270 million parameters: 170 million embedding parameters due to a large vocabulary size and 100 million for our transformer blocks. Thanks to the large vocabulary of 256k tokens, the model can handle specific and rare tokens, making it a strong base model to be further fine-tuned in specific domains and languages. Extreme energy efficiency: A key advantage of Gemma 3 270M is its low power consumption. Internal tests on a Pixel 9 Pro SoC show the INT4-quantized model used just 0.75% of the battery for 25 conversations, making it our most power-efficient Gemma model. Instruction following: An instruction-tuned model is released alongside a pre-trained checkpoint. While this model is not designed for complex conversational use cases, it‚Äôs a strong model that follows general instructions right out of the box. Production-ready quantization: Quantization-Aware Trained (QAT) checkpoints are available, enabling you to run the models at INT4 precision with minimal performance degradation, which is essential for deploying on resource-constrained devices.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-25 16:29:40+00:00						Industry
Gemini-2.5-Flash-Lite	Google DeepMind	2025-06-15	Best for high volume, cost efficient tasks				Unspecified unreleased		Unknown	Introducing 2.5 Flash-Lite, a thinking model for those looking for low cost and latency.  Upgrade to Gemini 2.5 2.5 Flash-Lite excels at high-volume, latency-sensitive tasks like translation and classification.  Thinking, enabled Experience improved reasoning and output quality with thinking mode and thinking budgets.  Superior latency Benefit from faster response times.  Tool use Utilize key Gemini 2.5 features including tool uses like Search and code execution.  Cost-efficient 2.5 Flash-Lite is our most cost-efficient 2.5 model yet.			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:56:58+00:00						Industry
Gemini-2.5-Flash	Google DeepMind	2025-04-17	Our powerful and most efficient workhorse model designed for speed and low-cost.				Unspecified unreleased		Unknown	Speed and value at scale Ideal for tasks like summarization, chat applications, data extraction, and captioning. Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost. Natively multimodal: Understands input across text, audio, images and video. Long context: Explore vast datasets with a 1-million token context window. Adaptive and budgeted thinking Adaptive controls and adjustable thinking budgets allow you to balance performance and cost. Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs. Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage. Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-04 05:11:23+00:00						Industry
Gemini-2.5-Pro	Google DeepMind	2025-03-25	Gemini 2.5: Our most intelligent AI model			Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.	Unspecified unreleased		Unknown	Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard ‚Äî which measures human preferences ‚Äî by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.  Gemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. We‚Äôll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-09-11 19:32:59+00:00						Industry
Gemma-3-27B	Google DeepMind	2025-03-12	Gemma 3 Technical Report 	27000000000	2.27E+24	6ND =  6 * 27B parameters * 14T training tokens = 2.268 √ó 10^24 FLOP	Unspecified unreleased		Confident	We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context ‚Äì at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-01 16:23:42+00:00						Industry
Gemma-3-12B	Google DeepMind	2025-03-12	Gemma 3 Technical Report 	12000000000	8.64E+23	6ND = 6 * 12B parameters * 12T training tokens = 8.64 √ó 10^23 FLOP	Unspecified unreleased		Confident	We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context ‚Äì at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-25 16:24:25+00:00						Industry
Gemma-3-4B	Google DeepMind	2025-03-12	Gemma 3 Technical Report 	4000000000	9.60E+22	6ND = 6 * 4B parameters * 4T training tokens = 9.6 √ó 10^22 FLOP	Unspecified unreleased		Confident	We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context ‚Äì at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-25 16:24:24+00:00						Industry
Gemma-3-1B	Google DeepMind	2025-03-12	Gemma 3 Technical Report 	1000000000	1.20E+22	6ND = 6 * 1B parameters * 2T training tokens = 1.2 √ó 10^22 FLOP	Unspecified unreleased		Confident	We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context ‚Äì at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-25 16:29:11+00:00						Industry
Gemini-2.0-Pro	Google DeepMind	2024-12-11	Our best model yet for coding performance and complex prompts			Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.	Unspecified unreleased		Unknown	Today, we‚Äôre releasing an experimental version of Gemini 2.0 Pro that responds to that feedback. It has the strongest coding performance and ability to handle complex prompts, with better understanding and reasoning of world knowledge, than any model we‚Äôve released so far. It comes with our largest context window at 2 million tokens, which enables it to comprehensively analyze and understand vast amounts of information, as well as the ability to call tools like Google Search and code execution.			Hosted access (no API)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-06-30 17:52:49+00:00						Industry
Gemma-2-9B	Google DeepMind	2024-06-24	Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.	9000000000	4.32E+23	"For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips"  6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP	Unspecified unreleased		Confident	Now we‚Äôre officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that‚Äôs now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:22:24+00:00						Industry
Gemma 2-27B	Google DeepMind	2024-06-24	Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.	27000000000	2.11E+24	"For the 27B model, we train on an 8x24x32 configuration of TPUv5p, totaling 6144 chips"  trained on 13T tokens  6ND = 6*27000000000*13000000000000=2.106e+24	Unspecified unreleased		Confident	Now we‚Äôre officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that‚Äôs now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:22:24+00:00						Industry
Gemma 2-2B	Google DeepMind	2024-06-24	Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.	2600000000	3.12E+22	"For the 2.6B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips"  6ND = 6 FLOP / token / parameter * 2600000000 parameters * 2000000000000 tokens = 3.12e+22 FLOP	Unspecified unreleased		Confident	Now we‚Äôre officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that‚Äôs now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-05 19:36:19+00:00						Industry
Gemini-Flash-1.5 	Google DeepMind	2024-05-10	Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context			"Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro."			Unknown	In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra‚Äôs state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5‚Äôs long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content. 			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:22:24+00:00						Industry
Gemini-1.5-Flash-8B	Google DeepMind	2024-05-10	Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context	8000000000					Confident	In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra‚Äôs state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5‚Äôs long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content. 			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-05-01 14:42:42+00:00						Industry
Gemini-pro-1.5	Google DeepMind	2024-02-15	Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context		1.58E+25	Training compute imputed from benchmark scores.	Unspecified unreleased		Speculative				API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-19 18:25:54+00:00						Industry
Gemini-2.0-Flash-Lite	Google DeepMind	2024-02-05	Gemini 2.0 Flash-Lite Model Card						Unknown	Description: Gemini 2.0 Flash-Lite is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power a new era of agentic systems. Gemini 2.0 Flash-Lite is Google‚Äôs most cost-efficient model, striking a balance between efficiency and quality targeting low-cost workflows. Inputs: Text strings (e.g., a question, a prompt, a document(s) to be summarized), images, audio, and video files, with a 1,048,576 token context window. Outputs: Text, with an 8,192 token output. Architecture: The Gemini 2.0 series builds upon the sparse Mixture-of-Experts (MoE) Transformer architecture (Clark et al., 2020; Fedus et al., 2021; Lepikhin et al., 2020; Riquelme et al., 2021; Shazeer et al., 2017; Zoph et al., 2022) used in Gemini 1.5. Key enhancements in Gemini 2.0 include refined architectural design and novel optimization methods, leading to substantial improvements in training stability and computational efficiency. Each model within the 2.0 family, including Gemini 2.0 Flash-Lite, is carefully designed and calibrated to achieve an optimal balance between quality and performance for their specific downstream applications.  			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:56:58+00:00						Industry
Gemma-3n	Google	2025-05-20	Announcing Gemma 3n preview: powerful, efficient, mobile-first AI	7850000000	5.18E+23	6 FLOP / parameter / token * 7.85 * 10^9 parameters * 11 * 10^12 tokens = 5.181e+23 FLOP	Unspecified unreleased		Confident	Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day ‚Äì your phones, tablets, and laptops.  To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung's System LSI business, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.  Gemma 3n is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of Gemini Nano, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland,Germany	2025-08-25 16:42:37+00:00						Industry
DeepSeek-R1-Distill-Llama-70B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	70000000000		base model compute: 6.8649768e+24 FLOP fine tune compute: 2.016e+22 FLOP			Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (unrestricted)	China	2025-08-05 19:08:20+00:00						Industry
DeepSeek-R1-Distill-Qwen-14B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	14800000000					Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (unrestricted)	China	2025-08-05 19:08:20+00:00						Industry
DeepSeek-R1-Distill-Qwen-1.5B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	1780000000					Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (unrestricted)	China	2025-08-05 19:08:20+00:00						Industry
DeepSeek-R1-Distill-Llama-8B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	8000000000		LLaMA 3.1-8B training compute = 1.224e+24 FLOP Fine-tuning compute = 2.3e21FLOP			Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (restricted use)	China	2025-08-11 14:56:58+00:00						Industry
DeepSeek-R1-Distill-Qwen-32B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	32000000000		Qwen2.5-32B: 3.51e+24 FLOP Fine-tune compute: 9.22e21 FLOP			Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (unrestricted)	China	2025-08-11 14:56:58+00:00						Industry
DeepSeek-R1-Distill-Qwen-7B	DeepSeek	2025-01-22	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	7000000000		Qwen2.5-7B: 8.2188e+23 FLOP Fine-tune compute: 2.01e21 FLOP			Speculative	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.	2		Open weights (unrestricted)	China	2025-08-11 14:56:58+00:00						Industry
DeepSeek-R1	DeepSeek	2025-01-20	DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning	671000000000	4.02E+24	Estimates by Ege Erdil in Gradient Updates: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1 "A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that‚Äôs valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek‚Äôs training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3‚Äôs 55 day training run ends up being around 23%."  6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining) 1.2e23 FLOP (post-training) 6.1e23 FLOP (fine-tuning)  Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24	Unspecified unreleased		Confident	We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.			Open weights (unrestricted)	China	2025-08-25 13:21:11+00:00						Industry
deepseek-chat-v3-0324	DeepSeek	2024-12-24	DeepSeek-V3 Technical Report	671000000000	3.41E+24	"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours."  6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.  We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8: 2.688M * 3600 * 1.513e15 * MFU = 3.2856e24  Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:  0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training  Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP			Confident	We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.			Open weights (restricted use)	China	2025-08-06 16:35:12+00:00		Paper on DeepSeek-V3				Industry
DeepSeek-V2.5	DeepSeek	2024-09-06	DeepSeek-V2.5	236000000000	1.79E+24	V2.5 is a merge of V2-coder and V2-chat V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training Unique steps: 8.2T + 6T = 14.2T FLOPs: 6 * 21B * 14.2T = 1.7892e24	GitHub,Common Crawl	Self-supervised learning	Confident				Open weights (restricted use)	China	2025-08-11 14:22:24+00:00		There is no paper to reference, no information about hardware used for training found in media.		36864000	Maximum batch size comes from training of V2-coder, which used long context training with 288 batches of 128k tokens = 36,864,000	Industry
Claude-4.1-Opus	Anthropic	2025-08-05	Claude Opus 4.1				Unspecified unreleased		Unknown	Today we're releasing Claude Opus 4.1, an upgrade to Claude Opus 4 on agentic tasks, real-world coding, and reasoning. We plan to release substantially larger improvements to our models in the coming weeks.  Opus 4.1 is now available to paid Claude users and in Claude Code. It's also on our API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing is the same as Opus 4.  Opus 4.1 advances our state-of-the-art coding performance to 74.5% on SWE-bench Verified. It also improves Claude‚Äôs in-depth research and data analysis skills, especially around detail tracking and agentic search.			API access	United States of America	2025-08-11 14:56:58+00:00						Industry
Claude-4-Opus	Anthropic	2025-05-22	Hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window			Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.	Unspecified unreleased		Unknown	Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours‚Äîdramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.			API access	United States of America	2025-08-11 14:56:58+00:00						Industry
claude-3.7-sonnet	Anthropic	2025-02-24	Claude 3.7 Sonnet		3.35E+25	https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0	Unspecified unreleased		Likely	Today, we‚Äôre announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.  Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. 			API access	United States of America	2025-08-11 14:52:15+00:00						Industry
Claude-3.5-Haiku	Anthropic	2024-10-22	Our fastest model, delivering advanced coding, tool use, and reasoning at an accessible price				Unspecified unreleased		Unknown	Claude 3.5 Haiku is the next generation of our fastest model. For a similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses even Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks. Claude 3.5 Haiku is particularly strong on coding tasks. For example, it scores 40.6% on SWE-bench Verified, outperforming many agents using publicly available state-of-the-art models‚Äîincluding the original Claude 3.5 Sonnet and GPT-4o.  With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, specialized sub-agent tasks, and generating personalized experiences from huge volumes of data‚Äîlike purchase history, pricing, or inventory records.  Claude 3.5 Haiku will be made available later this month across our first-party API, Amazon Bedrock, and Google Cloud‚Äôs Vertex AI‚Äîinitially as a text-only model and with image input to follow.			API access	United States of America	2025-08-11 14:52:15+00:00						Industry
Claude-3.5-Sonnet	Anthropic	2024-06-20	Claude 3.5 Sonnet		2.70E+25	Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls "Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors)."  Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/	Unspecified unreleased		Speculative	This addendum to our Claude 3 Model Card describes Claude 3.5 Sonnet, a new model which outperforms our previous most capable model, Claude 3 Opus, while operating faster and at a lower cost. Claude 3.5 Sonnet offers improved capabilities, including better coding and visual processing. Since it is an evolution of the Claude 3 model family, we are providing an addendum rather than a new model card. We provide updated key evaluations and results from our safety testing			API access	United States of America	2025-08-19 18:05:18+00:00						Industry
Claude-3-Haiku	Anthropic	2024-03-04	The Claude 3 Model Family: Opus, Sonnet, Haiku				Unspecified unreleased		Unknown	We introduce Claude 3, a new family of large multimodal models ‚Äì Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5]. 			API access	United States of America	2025-08-11 14:52:15+00:00						Industry
Claude-3-Sonnet	Anthropic	2024-03-04	The Claude 3 Model Family: Opus, Sonnet, Haiku				Unspecified unreleased		Unknown	We introduce Claude 3, a new family of large multimodal models ‚Äì Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5]. 			API access	United States of America	2025-08-11 14:52:15+00:00						Industry
Claude-3-Opus	Anthropic	2024-03-04	The Claude 3 Model Family: Opus, Sonnet, Haiku		1.64E+25	Training compute estimated from benchmark scores.	Unspecified unreleased		Speculative	We introduce Claude 3, a new family of large multimodal models ‚Äì Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5]. 			API access	United States of America	2025-08-19 16:47:29+00:00						Industry
Qwen3-235B-A22B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	235000000000	4.75E+24	6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP	Unspecified unreleased		Likely	Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.	1		Open weights (unrestricted)	China	2025-09-13 23:14:19+00:00						Industry
Qwen3-30B-A3B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	30000000000	6.48E+23	6 FLOP / parameter / token * 3*10^9 active parameters * 36000000000000 tokens = 6.48e+23 FLOP	Unspecified unreleased		Likely	Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.	1		Open weights (unrestricted)	China	2025-08-05 19:36:30+00:00						Industry
Qwen3-32B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	32800000000	7.08E+24	6 FLOP / parameter / token * 36 * 10^12 tokens * 32.8  * 10^9 parameters = 7.0848e+24 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-01 16:23:44+00:00						Industry
Qwen3-14B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	14800000000	3.20E+24	6 FLOP / parameter / token * 36 * 10^12 tokens * 14.8  * 10^9 parameters = 3.1968e+24 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-01 16:23:44+00:00						Industry
Qwen3-8B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	8200000000	1.77E+24	6 FLOP / parameter / token * 36 * 10^12 tokens * 8.2  * 10^9 parameters = 1.7712e+24 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-01 16:23:45+00:00						Industry
Qwen3-4B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	4000000000	8.64E+23	6 FLOP / parameter / token * 36 * 10^12 tokens * 4  * 10^9 parameters = 8.64e+23 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-01 16:23:45+00:00						Industry
Qwen3-1.7B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	1700000000	3.67E+23	6 FLOP / parameter / token * 36 * 10^12 tokens * 1.7  * 10^9 parameters = 3.672e+23 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-01 16:23:45+00:00						Industry
Qwen3-0.6B	Alibaba	2025-04-29	Qwen3: Think Deeper, Act Faster	600000000	1.30E+23	6 FLOP / parameter / token * 36 * 10^12 tokens * 0.6 * 10^9 parameters = 1.296e+23 FLOP	Unspecified unreleased		Confident	Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:  Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios. Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.	1		Open weights (unrestricted)	China	2025-08-05 19:36:31+00:00						Industry
Qwen2.5-Max	Alibaba	2025-01-28	Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model				Unspecified unreleased		Unknown	It is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. Today, we are excited to share the performance results of Qwen2.5-Max and announce the availability of its API through Alibaba Cloud. We also invite you to explore Qwen2.5-Max on Qwen Chat!			API access	China	2025-04-16 16:04:22+00:00						Industry
Qwen2.5 Instruct (7B)	Alibaba	2024-09-19	Qwen2.5: A Party of Foundation Models!	7610000000			Unspecified unreleased		Confident	Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:  Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Long-context Support up to 128K tokens and can generate up to 8K tokens. Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.			Open weights (unrestricted)	China	2025-08-04 19:29:39+00:00						Industry
Qwen2.5 Instruct (72B)	Alibaba	2024-09-19	Qwen2.5: A Party of Foundation Models!	72700000000	7.85E+24		Unspecified unreleased		Confident	Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:  Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Long-context Support up to 128K tokens and can generate up to 8K tokens. Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.			Open weights (restricted use)	China	2025-08-04 19:29:08+00:00		There is no paper to reference, no information about hardware used for training found in media.				Industry
Qwen2.5-3B	Alibaba	2024-09-19	Qwen2.5: A Party of Foundation Models!	3090000000	3.34E+23	Training dataset size was 18 trillion  6ND = 6 * 3.09 billion parameters * 18 trillion tokens = 3.3372e+23	Unspecified unreleased		Confident	In the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let‚Äôs get the party started!	1		Open weights (non-commercial)	China	2025-08-01 16:23:33+00:00						Industry
Qwen2.5-7B	Alibaba	2024-09-19	Qwen2.5: A Party of Foundation Models!	7610000000	8.22E+23	Training dataset size was 18 trillion  6ND = 6 * 7.61 billion parameters * 18 trillion tokens = 8.2188e+23	Unspecified unreleased		Confident	In the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let‚Äôs get the party started!  The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.	1		Open weights (unrestricted)	China	2025-08-01 16:23:33+00:00						Industry
Qwen2.5-1.5B	Alibaba	2024-09-19	Qwen2.5-LLM: Extending the boundary of LLMs	1540000000	1.66E+23	Training dataset size was 18 trillion  6ND = 6 * 1.54B billion parameters * 18 trillion tokens = 1.6632e+23	Unspecified unreleased		Confident	In the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let‚Äôs get the party started!	1		Open weights (unrestricted)	China	2025-08-01 16:23:34+00:00						Industry
Qwen2.5-14B	Alibaba	2024-09-19	Qwen2.5: A Party of Foundation Models!	14700000000	1.59E+24	Training dataset size was 18 trillion  6ND = 6 * 14.7 billion parameters * 18 trillion tokens = 1.59e24	Unspecified unreleased		Confident	In the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let‚Äôs get the party started!  The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.	1		Open weights (unrestricted)	China	2025-08-01 16:23:34+00:00						Industry
Qwen2-72B	Alibaba	2024-06-07	Hello Qwen2	72710000000	3.02E+24	72 billion params, 7 trillion tokens  6 * 72 billion * 7 trillion ~= 3.02e24	Unspecified unreleased	Self-supervised learning	Confident	After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you: - Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; - Having been trained on data in 27 additional languages besides English and Chinese; - State-of-the-art performance in a large number of benchmark evaluations; - Significantly improved performance in coding and mathematics; - Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.  (Technical report to follow)			Open weights (unrestricted)	China	2025-08-11 14:22:24+00:00		There is no paper to reference, no information about hardware used for training found in media.				Industry
Qwen2-7B	Alibaba	2024-06-07	Hello Qwen2	7000000000	2.94E+23	7 billion params, 7 trillion tokens  6 FLOP * 7 billion * 7 trillion ~= 2.94e23 FLOP	Unspecified unreleased	Self-supervised learning	Confident	This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.			Open weights (unrestricted)	China	2025-08-09 00:08:29+00:00		There is no paper to reference, no information about hardware used for training found in media.				Industry
Qwen2-57B-A14B	Alibaba	2024-06-07	Hello Qwen2	57000000000	3.78E+23	"For MoE models, 57B-A14B denotes that the model has 57B parameters in total and for each token 14B parameters are active" (page 5)  C ~= 6 FLOP * 14e9 * 4.5e12 = 3.78e23	Unspecified unreleased	Self-supervised learning	Confident	This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.			Open weights (unrestricted)	China	2025-08-09 00:08:54+00:00						Industry
Qwen2-1.5B	Alibaba	2024-06-07	Hello Qwen2	1500000000	6.30E+22	6 FLOP / parameter / token * 1.5 * 10^9  parameters * 7 * 10^12 tokens = 6.3e+22 FLOP	Unspecified unreleased		Confident	This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.			Open weights (unrestricted)	China	2025-08-22 07:54:09+00:00						Industry
Qwen-72B	Alibaba	2023-11-30		72000000000	1.30E+24	72 billion params, 3 trillion tokens 72b * 3T * 6 = 1.3e24			Confident	Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.			Open weights (restricted use)	China	2025-09-09 16:14:30+00:00		The paper does not mention any hardware, GPUs or any information regarding the hardware used.		4000000	Table 1 https://arxiv.org/abs/2309.16609 (this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)	Industry
Sonar-Reasoning-Pro	Perplexity	2025-03-25	Premier reasoning offering powered by DeepSeek R1.				Unspecified unreleased		Unknown	Advanced CoT reasoning  Enhanced chain-of-thought reasoning, plus real-time internet search and citations  US-based  Sonar Reasoning Pro is uncensored and hosted in US datacenters  Data privacy  No training on customer data, just like all Sonar products  In-depth answers with 2x more citations than Sonar Reasoning on average  Advanced information retrieval from multiple web searches  Self-serve API access  Get started immediately			API access	United States of America	2025-08-11 14:56:58+00:00						Industry
Claude-Sonnet-4	Anthropic	2025-05-22	Hybrid reasoning model with superior intelligence for high-volume use cases, and 200K context window			Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.	Unspecified unreleased		Unknown	Claude Sonnet 4 can understand nuanced instructions and context, recognize and correct its own mistakes, and create sophisticated analysis and insights from complex data. Combined with superior coding, vision, and writing skills, you can use Claude Sonnet 4 for a variety of use cases.  Claude Sonnet 4 can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model thinks for.			API access	United States of America	2025-08-11 14:56:58+00:00						Industry
GPT-4	OpenAI	2023-03-15	GPT-4 Technical Report	1800000000000	2.10E+25	90% CI: 8.2E+24 to 4.4E+25  NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.  Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing	Unspecified unreleased	Self-supervised learning	Likely	We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.	2		API access	United States of America	2025-08-25 21:26:26+00:00						Industry
gpt-3.5-turbo-0125	OpenAI	2022-11-30	A fast, inexpensive model for simple tasks	20000000000			Unspecified unreleased		Speculative	GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.			API access	United States of America	2025-08-15 20:38:25+00:00	Azure AI					Industry
GPT-4o	OpenAI	2024-05-13	Hello GPT-4o		3.81E+25	Training compute estimated from benchmark scores.	Unspecified unreleased		Speculative	We‚Äôre announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.  GPT-4o (‚Äúo‚Äù for ‚Äúomni‚Äù) is a step towards much more natural human-computer interaction‚Äîit accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.			API access	United States of America	2025-08-19 17:52:10+00:00						Industry
Mistral-7B-instruct	Mistral AI	2023-10-10	Mistral 7B	7000000000			Unspecified unreleased		Confident	We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.			Open weights (unrestricted)	France	2025-08-11 14:22:24+00:00						Industry
Gemma-7B-it	Google	2024-02-24		8540000000	3.07E+23	6ND = 6*6000000000000*8540000000=3.0744e+23	Unspecified unreleased		Confident	This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.  Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with "Sure,".  We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.			Open weights (restricted use)	United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland,Germany	2025-08-05 19:36:12+00:00						Industry
OpenChat-7B	Tsinghua University	2023-11-01		7000000000					Confident	OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.			Open weights (unrestricted)	China	2024-09-05 18:08:31+00:00						Academia
Claude-3-Haiku:beta	Anthropic	2024-03-04	The Claude 3 Model Family: Opus, Sonnet, Haiku				Unspecified unreleased		Unknown	We introduce Claude 3, a new family of large multimodal models ‚Äì Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5]. 			API access	United States of America	2025-08-11 14:52:15+00:00						Industry
palm-2-chat-bison-32k	Google	2023-05-10	PaLM 2 models						Unknown	PaLM 2 is a family of language models, optimized for ease of use on key developer use cases. The PaLM family of models includes variations trained for text and chat generation as well as text embeddings. This guide provides information about each variation to help you decide which is the best fit for your use case.			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland,Germany	2025-08-06 18:36:06+00:00						Industry
Gemini-2.0-Flash-Lite-001	Google DeepMind	2024-02-05	Gemini 2.0 Flash-Lite Model Card						Unknown	Description: Gemini 2.0 Flash-Lite is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power a new era of agentic systems. Gemini 2.0 Flash-Lite is Google‚Äôs most cost-efficient model, striking a balance between efficiency and quality targeting low-cost workflows. Inputs: Text strings (e.g., a question, a prompt, a document(s) to be summarized), images, audio, and video files, with a 1,048,576 token context window. Outputs: Text, with an 8,192 token output. Architecture: The Gemini 2.0 series builds upon the sparse Mixture-of-Experts (MoE) Transformer architecture (Clark et al., 2020; Fedus et al., 2021; Lepikhin et al., 2020; Riquelme et al., 2021; Shazeer et al., 2017; Zoph et al., 2022) used in Gemini 1.5. Key enhancements in Gemini 2.0 include refined architectural design and novel optimization methods, leading to substantial improvements in training stability and computational efficiency. Each model within the 2.0 family, including Gemini 2.0 Flash-Lite, is carefully designed and calibrated to achieve an optimal balance between quality and performance for their specific downstream applications.  			API access	United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational	2025-08-11 14:56:58+00:00						Industry
phi-3-mini-128k-instruct	Microsoft	2024-04-23	Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone	3800000000	7.52E+22	counting operations: 6√ó3.3√ó10^12 tokens √ó3.8√ó10^9 parameters ‚âà7.524√ó10^22 FLOPS hardware estimate: 7 days √ó24 hours / day√ó3600 sec / hour *989,000,000,000,000 FLOP/s*512 GPUs*0.3 [assumed utilization]=9.187540992√ó10^22	Phi-3 Dataset		Confident	We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).			Open weights (unrestricted)	United States of America,Multinational,India,Belgium	2025-08-05 19:36:15+00:00						Industry
Llama-3-70B-instruct:nitro	Meta AI	2024-04-18	Introducing Meta Llama 3: The most capable openly available LLM to date	70000000000	7.86E+24	Arithmetic calculation: 6 * 15T tokens * 70B parameters = 6.3e24  GPU calculation: https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one). 990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24  Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24	Llama 3 dataset		Confident				Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
llama-3-8b-instruct:nitro	Meta AI	2024-04-18	Introducing Meta Llama 3: The most capable openly available LLM to date	8000000000	7.20E+23	Counting operations 15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2√ó10^23 FLOP  GPU calculation 400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872√ó10^24  (it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)	Llama 3 dataset		Confident	Today, we‚Äôre introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm. We‚Äôre dedicated to developing Llama 3 in a responsible way, and we‚Äôre offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2. In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we‚Äôll share the Llama 3 research paper. Meta AI, built with Llama 3 technology, is now one of the world‚Äôs leading AI assistants that can boost your intelligence and lighten your load‚Äîhelping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.			Open weights (restricted use)	United States of America	2025-08-11 14:22:24+00:00						Industry
mixtral-8x7b-instruct:nitro	Mistral AI	2023-10-10	Mistral 7B	7000000000			Unspecified unreleased		Confident	We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.			Open weights (unrestricted)	France	2025-08-11 14:22:24+00:00						Industry